How to run hello-world example with/without container

Wei Huang
NOAA, EPIC, I2X Technologies

1. hello-world.c
--------------------------------------------------------------------------------------------------
/* Program hello_mpi.c  */
#include <stdio.h>
#include <string.h>
#include <mpi.h>

int main(int argc, char **argv)
{
  char message[20];
  int  i, rank, size, tag = 99;
  MPI_Status status;

  // Get the name of the processor
  char processor_name[MPI_MAX_PROCESSOR_NAME];
  int name_len;

  MPI_Init(&argc, &argv);
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Get_processor_name(processor_name, &name_len);

  if (rank == 0)
  {
      strcpy(message, "Hello, world");
      for (i = 1; i < size; i++)
      {
        MPI_Send(message, 13, MPI_CHAR, i, tag, MPI_COMM_WORLD);

        // Print off a hello world message
        printf("%s sent to %d from processor %s, rank %d out of %d processors\n",
                message, i, processor_name, rank, size);
      }

      if ( 1 == size )
      {
        // Print off a hello world message
        printf("%s on rank %d at processor %s, rank %d out of %d processors\n",
                message, rank, processor_name, rank, size);
      }
  }
  else
  {
      MPI_Recv(message, 20, MPI_CHAR, 0, tag, MPI_COMM_WORLD, &status);

      // printf( "Message from process %d : %.13s\n", rank, message);

      // Print off a hello world message
      printf("%s received at processor %s, rank %d out of %d processors\n",
              message, processor_name, rank, size);
  }

  MPI_Finalize();
}


--------------------------------------------------------------------------------------------------
2. run on host (derecho):

--------------------------------------------------------------------------------------------------
with script: host.pbs
--------------------------------------------------------------------------------------------------
#!/bin/bash
# hello.pbs - Submit with qsub hello.pbs
#PBS -N host-test
#PBS -A NRAL0032
#PBS -q develop
#PBS -l walltime=00:10:00
#PBS -l select=1:mpiprocs=16:ncpus=16
#PBS -o log.host
#PBS -e err.host

source /glade/u/apps/derecho/24.12/spack/opt/spack/lmod/8.7.37/gcc/12.4.0/nr3e/lmod/lmod/init/bash
module --force purge
module load ncarenv/24.12 intel-oneapi/2024.2.1 cray-mpich/8.1.29

cd /gpfs/csfs1/work/huangwei/src/container-hello-world

mpicc hello-world.c -o host-hello-world.x

mpiexec -n 16 ./host-hello-world.x


--------------------------------------------------------------------------------------------------
3. compile in container

use this script to shell into container:
--------------------------------------------------------------------------------------------------
shell-in-container.sh
--------------------------------------------------------------------------------------------------
#!/bin/bash

set -x

source /glade/u/apps/derecho/24.12/spack/opt/spack/lmod/8.7.37/gcc/12.4.0/nr3e/lmod/lmod/init/bash
module --force purge
module load ncarenv/24.12 intel-oneapi/2024.2.1 cray-mpich/8.1.29

module load libfabric/1.15.2.0
module load apptainer

sif=ubuntu22.04-intel-ufs-env-v1.9.2.img
img=/glade/u/home/huangwei/containers/${sif}
bindings="-B /glade -B /gpfs -B /lustre -B /opt/cray"

apptainer shell -e ${bindings} "${img}"

	a. Login to a compute node
	b. run "shell-in-container.sh"
	c. within container run "mpiicx hello-world.c -o hello-world.x"
        d. run "mpirun -n 4 ./hello-world.x"

--------------------------------------------------------------------------------------------------
NOTE: mpirun will fail on login node.


4. Run hello world with container

--------------------------------------------------------------------------------------------------
   a. run within container: in-container.sh
--------------------------------------------------------------------------------------------------
#!/bin/bash
#PBS -N container-test
#PBS -A NRAL0032
#PBS -q develop
#PBS -l walltime=00:10:00
#PBS -l select=1:mpiprocs=4:ncpus=4
#PBS -o log.hello
#PBS -e err.hello

# Load modules
source /glade/u/apps/derecho/24.12/spack/opt/spack/lmod/8.7.37/gcc/12.4.0/nr3e/lmod/lmod/init/bash
module --force purge
module load ncarenv/25.10
module load apptainer/1.4.1

apptainer exec \
    --cleanenv \
    -B /glade -B /gpfs -B /lustre -B /opt/cray \
    /glade/u/home/huangwei/containers/ubuntu22.04-intel-ufs-env-v1.9.2.img \
    mpirun -n 4 /gpfs/csfs1/work/huangwei/src/container-hello-world/hello-world.x


--------------------------------------------------------------------------------------------------
   b. [mpi]run out-side container
--------------------------------------------------------------------------------------------------
#!/bin/bash
# hello.pbs - Submit with qsub hello.pbs
#PBS -N container-test
#PBS -A NRAL0032
#PBS -q develop
#PBS -l walltime=00:10:00
#PBS -l select=1:mpiprocs=4:ncpus=4
#PBS -o log.hello
#PBS -e err.hello

source /glade/u/apps/derecho/24.12/spack/opt/spack/lmod/8.7.37/gcc/12.4.0/nr3e/lmod/lmod/init/bash
module --force purge
module load ncarenv/24.12 intel-oneapi/2024.2.1 cray-mpich/8.1.29
module load libfabric/1.15.2.0
module load apptainer

echo "=== run-hello-world.sh ==="
mpiexec -n 4 apptainer exec \
    --cleanenv \
    -B /glade -B /gpfs -B /lustre -B /opt/cray \
    -B /opt/cray/pe \
    -B /opt/cray/libfabric \
    /glade/u/home/huangwei/containers/ubuntu22.04-intel-ufs-env-v1.9.2.img \
    /gpfs/csfs1/work/huangwei/src/container-hello-world/run-hello-world.sh

--------------------------------------------------------------------------------------------------
    c. /gpfs/csfs1/work/huangwei/src/container-hello-world/run-hello-world.sh
--------------------------------------------------------------------------------------------------
#!/bin/bash

#. /opt/intel/oneapi/setvars.sh --force >& /dev/null

 cd /gpfs/csfs1/work/huangwei/src/container-hello-world

 ./hello-world.x


--------------------------------------------------------------------------------------------------
  c. use a wrapper: run-container.pbs
--------------------------------------------------------------------------------------------------
#!/bin/bash
#PBS -N container-test
#PBS -A NRAL0032
#PBS -q develop
#PBS -l walltime=00:10:00
#PBS -l select=1:mpiprocs=16:ncpus=16
#PBS -o log.container
#PBS -e err.container

#source /glade/u/apps/derecho/24.12/spack/opt/spack/lmod/8.7.37/gcc/12.4.0/nr3e/lmod/lmod/init/bash
module --force purge
module load ncarenv/24.12 intel/2024.2.1 cray-mpich/8.1.29 apptainer/1.3.4

echo "=== run-hello-world ==="
#mpiexec -n 16 -ppn 8 --cpu-bind depth --depth 1 \
mpiexec -n 16 \
	/gpfs/csfs1/work/huangwei/src/container-hello-world/run-hello-world.wrapper \
	/gpfs/csfs1/work/huangwei/src/container-hello-world/run-hello-world.sh


--------------------------------------------------------------------------------------------------
/gpfs/csfs1/work/huangwei/src/container-hello-world/run-hello-world.wrapper:
--------------------------------------------------------------------------------------------------
#!/bin/bash

sif=ubuntu22.04-intel-ufs-env-v1.9.2.img
img=/glade/u/home/huangwei/containers/${sif}
bindings="-B /glade -B /gpfs -B /opt/cray -B /run -B /usr/lib64:/host/usr/lib64"

apptainer exec ${bindings} ${img} "$@"

--------------------------------------------------------------------------------------------------
/gpfs/csfs1/work/huangwei/src/container-hello-world/run-hello-world.sh:
--------------------------------------------------------------------------------------------------
#!/bin/bash

source /usr/lmod/lmod/init/bash
module use "/glade/work/huangwei/src/container/global-workflow-cloud/sorc/ufs_model.fd/modulefiles"
module load ufs_container.intel

export LD_LIBRARY_PATH=${CRAY_MPICH_DIR}/lib-abi-mpich:/opt/cray/pe/lib64:$LD_LIBRARY_PATH:/host/usr/lib64

/gpfs/csfs1/work/huangwei/src/container-hello-world/hello-world.x "$@"

--------------------------------------------------------------------------------------------------
output
--------------------------------------------------------------------------------------------------
=== run-hello-world ===
Hello, world received at processor dec2449, rank 14 out of 16 processors
Hello, world received at processor dec2449, rank 15 out of 16 processors
Hello, world sent to 1 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 2 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 3 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 4 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 5 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 6 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 7 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 8 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 9 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 10 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 11 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 12 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 13 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 14 from processor dec2449, rank 0 out of 16 processors
Hello, world sent to 15 from processor dec2449, rank 0 out of 16 processors
Hello, world received at processor dec2449, rank 1 out of 16 processors
Hello, world received at processor dec2449, rank 2 out of 16 processors
Hello, world received at processor dec2449, rank 3 out of 16 processors
Hello, world received at processor dec2449, rank 4 out of 16 processors
Hello, world received at processor dec2449, rank 5 out of 16 processors
Hello, world received at processor dec2449, rank 6 out of 16 processors
Hello, world received at processor dec2449, rank 7 out of 16 processors
Hello, world received at processor dec2449, rank 8 out of 16 processors
Hello, world received at processor dec2449, rank 10 out of 16 processors
Hello, world received at processor dec2449, rank 11 out of 16 processors
Hello, world received at processor dec2449, rank 12 out of 16 processors
Hello, world received at processor dec2449, rank 13 out of 16 processors
Hello, world received at processor dec2449, rank 9 out of 16 processors
